# ğŸ¤– Agentic Evaluation Framework  
A Project for the e6data x IIT BHU Hackathon 2025 by **The Pareto Crew**  

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)  
[![Colab](https://img.shields.io/badge/Google%20Colab-Ready-orange.svg)](https://colab.research.google.com/)  
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)  

---

## ğŸš© The Problem: The Scaling Challenge of AI Evaluation  

As AI agents become more powerful and widespread, *evaluating their performance* is a critical bottleneck.  

Manual evaluation is:  
- âŒ Slow  
- âŒ Expensive  
- âŒ Subjective  
- âŒ Impossible to scale  

When a company has hundreds of agents producing thousands of responses, how can they efficiently ensure that the agents:  
- âœ… Follow instructions precisely  
- âœ… Avoid factual hallucinations  
- âœ… Provide coherent and useful responses  
- âœ… Do not make unwarranted assumptions  

Without an automated and robust solution, *building trustworthy AI is not feasible*.  

---

## ğŸ’¡ Our Solution: An Automated Evaluation & Comparison Platform  

We developed the *Agentic Evaluation Framework, an **interactive platform* designed to solve the problem of large-scale agent evaluation.  

It provides developers and researchers with tools to:  
- ğŸ” Automatically score agent responses  
- âš– Compare different models head-to-head  
- ğŸ“ˆ Track performance over time  

Our framework directly addresses the hackathon challenge by accepting prompts, responses, and metadata to output an *interpretable performance report*.  

---

## âœ¨ Key Features  

Built as an *interactive Google Colab notebook*, our framework includes:  

- âš¡ *Batch Processing Engine*  
  Process thousands of responses from a CSV file, scoring each one across four key dimensions:  
  Instruction-following, coherence, assumption control, hallucination detection.  

- ğŸ¯ *Live "Head-to-Head" Demo*  
  Input a single prompt â†’ two AI models generate responses â†’ both evaluated & displayed *side-by-side in real-time*.  

- ğŸ“Š *Performance Dashboard*  
  A visualization tab with a *ranked leaderboard* of evaluated agents, offering a clear overview of performance.  

- ğŸ§© *Metadata Integration*  
  Evaluation process accepts metadata for *context-aware judgments*.  

---

## ğŸ›  Technical Methodology  

- *Core Technology:* Python (Google Colab, T4 GPU)  
- *Local LLMs with Ollama:* llama3:8b and phi3:mini (open-source, zero cost, highly flexible)  
- *Asynchronous Processing:* asyncio executes generation + evaluation calls in parallel â†’ *faster results*  
- *Interactive UI:* Built with ipywidgets for a *responsive Colab experience*  

---

## âš™ Setup & Usage Instructions  

### ğŸ”§ 1. Open in Google Colab  
- Open Agentic_AI.py in Colab  
- Copy-paste code into a single cell  

### âš™ 2. Set the Runtime  
- Go to Runtime â†’ Change runtime type  
- Select *T4 GPU*  

### ğŸ“¦ 3. Run Setup  
- Run *Cell 1*  
- Installs Ollama, pulls required models, installs dependencies  
- Takes a few minutes  

### ğŸ“‚ 4. Upload Data  
- Use Colab file browser (left panel)  
- Upload final_report_with_scores.csv  
- Required for Performance Dashboard  

### ğŸš€ 5. Launch the App  
- Run *Cell 2*  
- Opens interactive *Head-to-Head Demo* + *Performance Dashboard*  

---

## ğŸ“Š Example Workflow  

1. Upload a dataset with agent responses  
2. Run *batch evaluation* â†’ get scores across 4 metrics  
3. View *Performance Dashboard* â†’ ranked leaderboard  
4. Use *Head-to-Head Demo* â†’ real-time model comparison  

---

## ğŸ“œ License  

This project is licensed under the [MIT License](LICENSE).  

---

## ğŸ™Œ Acknowledgements  

Developed by *The Pareto Crew* for the e6data x IIT BHU HackathonÂ 2025.